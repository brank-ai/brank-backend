# brank-backend — Cursor Agent Rules

You are an autonomous coding agent working on brank-backend. Follow these rules before writing any code.

## Project Context (Must Remember)

### What is brank-backend?
- **Purpose**: Backend API for Brank - helps brands understand how LLMs perceive them
- **Main Endpoint**: `GET /metric?website=<brand_website>`
- **Goal**: Calculate brand visibility metrics across 4 major LLMs

### The /metric Pipeline (4 Steps)

When `/metric?website=samsung.com` is called:

**Step 1: Prompt Generation**
- Use ChatGPT to generate N user questions where the brand could be relevant
- N is configured via `PROMPTS_N` environment variable
- Example for Samsung:
  - "What is the best mobile under 500 USD?"
  - "I need a phone with good camera features. What brands do you suggest?"

**Step 2: Fetch LLM Responses**
- Send all N prompts to 4 LLMs in parallel:
  - ChatGPT (OpenAI)
  - Gemini (Google)
  - Grok (xAI)
  - Perplexity
- API keys stored as: `CHATGPT_API_KEY`, `GEMINI_API_KEY`, `GROK_API_KEY`, `PERPLEXITY_API_KEY`

**Step 3: Process Responses**
- For each response from each LLM:
  - Extract `brands_list`: ordered list of brands mentioned
  - Extract `citation_list`: all URLs/links in the response
- Store in `responses` table with `llm_name`

**Step 4: Calculate Metrics**
- For each LLM independently, calculate 4 metrics:
  1. **brandRank**: Average position of requested brand across responses
  2. **citationsList**: Top 5 URLs by citation percentage
  3. **mentionRate**: Percentage of responses that mention the brand
  4. **sentimentScore**: 0-100 sentiment score for the brand
- Each LLM returns its own set of 4 metrics

### Caching Rule (24 Hours)
- Before starting pipeline, check `metrics.updated_at` for the brand
- If metrics exist and `updated_at` < 24 hours ago → return cached metrics
- Else → run full pipeline and update database

---

## Tech Stack

- **Language**: Python 3.11+
- **Framework**: Flask (use application factory pattern)
- **Dependency Manager**: uv + pyproject.toml
- **Database**: SQLAlchemy + Alembic for migrations
- **Testing**: pytest with mocked external APIs
- **Formatting**: Black (88 chars), isort
- **Type Checking**: mypy

---

## Database Schema

### 1. brands
```
brand_id (PK, UUID)
name (string)
website (string, unique)
created_at (timestamp)
updated_at (timestamp)
```

### 2. prompts
```
prompt_id (PK, UUID)
brand_id (FK → brands.brand_id)
prompt (text)
created_at (timestamp)
updated_at (timestamp)
```

### 3. responses
```
response_id (PK, UUID)
prompt_id (FK → prompts.prompt_id)
llm_name (string: 'chatgpt'|'gemini'|'grok'|'perplexity')
answer (text)
brands_list (JSON array of strings, ordered)
citation_list (JSON array of URLs)
created_at (timestamp)
updated_at (timestamp)
```

### 4. metrics
```
metric_id (PK, UUID)
brand_id (FK → brands.brand_id)
llm_name (string: 'chatgpt'|'gemini'|'grok'|'perplexity')
mention_rate (float, 0-1)
citations_list (JSON array of {url, percentage})
sentiment_score (float, 0-100)
brand_rank (float, nullable)
created_at (timestamp)
updated_at (timestamp)
```

### 5. time_profiling
```
profile_id (PK, UUID)
brand_id (FK → brands.brand_id)
request_id (UUID, for tracking individual requests)
prompt_generation_time (float, seconds)
fetching_llm_response_time (float, seconds)
processing_response_time (float, seconds)
metrics_calculation_time (float, seconds)
created_at (timestamp)
```

**Indexes**: Create on `website`, `brand_id`, `(brand_id, llm_name)`, `(brand_id, updated_at)`

---

## Architecture (Mandatory Layer Structure)

```
brank-backend/
├── api/              # Flask routes, request/response schemas
│   ├── __init__.py
│   ├── routes.py     # /metric endpoint
│   └── schemas.py    # Pydantic models for validation
├── services/         # Business logic orchestration
│   ├── __init__.py
│   ├── metric_service.py      # Main pipeline orchestrator
│   └── cache_service.py       # 24h cache logic
├── llm_clients/      # LLM provider integrations
│   ├── __init__.py
│   ├── base.py       # Abstract base client
│   ├── chatgpt.py
│   ├── gemini.py
│   ├── grok.py
│   └── perplexity.py
├── extractors/       # Data extraction logic
│   ├── __init__.py
│   ├── brand_extractor.py     # Extract brands from text
│   ├── citation_extractor.py  # Extract URLs
│   └── sentiment_analyzer.py  # Calculate sentiment
├── db/               # Database layer
│   ├── __init__.py
│   ├── models.py     # SQLAlchemy models
│   ├── repositories/ # Data access objects
│   └── migrations/   # Alembic migrations
├── utils/            # Shared utilities
│   ├── __init__.py
│   ├── logger.py     # Structured logging
│   ├── timing.py     # Performance timing decorator
│   └── retry.py      # Retry logic with backoff
├── config.py         # Pydantic Settings
├── app.py            # Flask app factory
└── tests/            # Mirror structure of source
```

**Rules**:
- API routes must NOT contain business logic
- Services must NOT import Flask objects
- DB access only through repositories
- All external calls isolated behind client interfaces

---

## CRITICAL: Dependency Injection (Prevent Memory Leaks)

**ALWAYS pass dependencies explicitly as function/method parameters**

❌ **NEVER DO THIS**:
```python
# Global singleton - causes memory leaks!
db_session = create_session()
llm_client = ChatGPTClient()

def calculate_metrics(brand_id: str):
    # Using globals
    data = db_session.query(...)
```

✅ **ALWAYS DO THIS**:
```python
def calculate_metrics(
    brand_id: str,
    db_session: Session,
    llm_clients: dict[str, LLMClient],
    logger: Logger
) -> dict:
    # Explicit dependencies
    data = db_session.query(...)
```

**Flask Patterns**:
- Use application factory: `def create_app() -> Flask`
- Use `flask.g` or request-scoped objects for DB sessions
- SQLAlchemy: use `scoped_session` or Flask-SQLAlchemy
- Create LLM clients per request or use proper connection pooling

---

## Metric Definitions (Calculate Per LLM)

Each of the 4 LLMs returns independent metrics. For a brand with N prompts:

### 1. brandRank
- **Definition**: Average position where brand appears in responses
- **Algorithm**:
  1. For each response, find 1-based index of brand in `brands_list`
  2. If brand not in `brands_list` → skip that response
  3. Average all found positions
  4. If brand never appears → `null`
- **Example**: Brand appears at positions [2, 1, 3, 1] → brandRank = 1.75
- **Type**: `float | null`

### 2. citationsList
- **Definition**: Top 5 most-cited URLs by this LLM
- **Algorithm**:
  1. Count occurrences of each URL across all N responses
  2. Calculate `citation_percentage = (count / N) * 100`
  3. Sort by percentage (descending), then by count
  4. Return top 5
- **Format**: `[{"url": "https://...", "percentage": 80.0}, ...]`
- **Type**: `list[dict]` (max 5 items)

### 3. mentionRate
- **Definition**: Percentage of responses that mention the brand
- **Algorithm**: `(responses_with_brand / total_responses)`
- **Example**: Brand in 8 of 10 responses → 0.8
- **Type**: `float` (0.0 to 1.0)

### 4. sentimentScore
- **Definition**: Overall sentiment toward brand (0=very negative, 100=very positive)
- **Algorithm**: Use sentiment analysis on responses mentioning the brand
- **Type**: `float` (0 to 100)

**API Response Structure**:
```json
{
  "brand_id": "uuid",
  "website": "samsung.com",
  "cached": false,
  "metrics": {
    "chatgpt": {
      "brandRank": 1.75,
      "citationsList": [{"url": "...", "percentage": 80.0}],
      "mentionRate": 0.8,
      "sentimentScore": 75.5
    },
    "gemini": { ... },
    "grok": { ... },
    "perplexity": { ... }
  },
  "computed_at": "2026-01-09T10:30:00Z"
}
```

---

## Code Standards

### Python Style
- **Type hints**: Required on all public functions/methods
- **Formatting**: Black (88 chars), isort for imports
- **Naming**: 
  - `snake_case` for functions, variables, modules
  - `PascalCase` for classes
  - `UPPER_CASE` for constants
- **Docstrings**: Google style for all public APIs
- **Error handling**: Use specific exceptions, never bare `except:`

### Example Function Signature
```python
def calculate_brand_rank(
    brand_name: str,
    responses: list[Response],
    logger: Logger
) -> float | None:
    """Calculate average rank of brand across LLM responses.
    
    Args:
        brand_name: Name of brand to search for (case-insensitive)
        responses: List of Response objects with brands_list
        logger: Logger instance for debugging
        
    Returns:
        Average rank (1-based) or None if brand never appears
        
    Raises:
        ValueError: If responses list is empty
    """
    pass
```

### Configuration Management
Use Pydantic Settings:
```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    # LLM API Keys
    chatgpt_api_key: str
    gemini_api_key: str
    grok_api_key: str
    perplexity_api_key: str
    
    # Pipeline config
    prompts_n: int = 10
    
    # Database
    database_url: str
    
    # Timeouts
    llm_timeout_seconds: int = 30
    max_retries: int = 3
    
    class Config:
        env_file = ".env"
        case_sensitive = False
```

---

## Performance & Resilience

### Concurrency
- **Fan-out to LLMs**: Use threading or async to query 4 LLMs in parallel
- **Rate limiting**: Respect provider limits, use exponential backoff
- **Timeout**: Every network call must have timeout (default 30s)

### Retry Logic
```python
@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=retry_if_exception_type((Timeout, ConnectionError))
)
def call_llm_api(...):
    pass
```

### Error Handling
- **Partial failures**: If 1 LLM fails, still return metrics for other 3
- **Graceful degradation**: Log errors, include status per LLM in response
- **Never fail entire request** for single LLM timeout

---

## Security Rules

### Environment Variables
- **NEVER** hardcode API keys
- **NEVER** commit `.env` file
- Validate all env vars at startup (fail fast)

### Input Validation
- Validate `website` parameter (URL format)
- Sanitize before database queries
- Validate LLM responses before parsing

### Logging
- **DO** log: request_id, brand_id, step names, durations, errors
- **DON'T** log: API keys, full LLM prompts/responses (may contain sensitive data)

---

## Testing Requirements

### Unit Tests (Required)
- **Extractors**: brand/link extraction logic
- **Metrics**: calculation functions (brandRank, citationsList, etc.)
- **Cache logic**: 24h window checks

### Integration Tests
- `/metric` endpoint with mocked LLM clients
- Database operations (use test DB or transactions)
- Partial failure scenarios (1 LLM times out)

### Test Structure
```python
def test_brand_rank_calculation():
    # Given
    responses = [
        Response(brands_list=["Apple", "Samsung"]),
        Response(brands_list=["Samsung", "Google"]),
    ]
    
    # When
    rank = calculate_brand_rank("Samsung", responses)
    
    # Then
    assert rank == 1.5  # (2 + 1) / 2
```

### Mocking
```python
@pytest.fixture
def mock_llm_client():
    client = Mock(spec=LLMClient)
    client.query.return_value = "Samsung and Apple are great..."
    return client
```

---

## Observability

### Structured Logging
```python
logger.info(
    "Step completed",
    extra={
        "request_id": request_id,
        "brand_id": brand_id,
        "step": "prompt_generation",
        "duration_ms": 1250,
        "prompt_count": 10
    }
)
```

### Timing
- Measure and log duration of each pipeline step
- Persist to `time_profiling` table
- Use decorators for consistent timing

### Metrics to Track
- Request duration (total and per step)
- LLM API success/failure rates
- Cache hit rate
- Database query times

---

## Agent Workflow (How You Should Work)

### Before Coding
1. Read existing files to understand current patterns
2. Check if similar functionality exists
3. Identify which layer the change belongs to (api/services/db/etc.)
4. Plan which files need changes

### While Coding
1. Make small, focused changes
2. Follow existing code style
3. Add type hints and docstrings
4. Handle errors explicitly
5. Use dependency injection

### After Coding
1. Run formatters (Black, isort)
2. Check type hints (mypy)
3. Add/update tests
4. Verify no new linter errors

---

## Common Patterns

### Flask Route
```python
@bp.route("/metric", methods=["GET"])
def get_metrics():
    # 1. Validate input
    website = request.args.get("website")
    if not website:
        return jsonify({"error": "website parameter required"}), 400
    
    # 2. Get dependencies
    db_session = get_db_session()
    llm_clients = get_llm_clients()
    
    # 3. Call service (no business logic here!)
    try:
        result = metric_service.get_or_compute_metrics(
            website=website,
            db_session=db_session,
            llm_clients=llm_clients
        )
        return jsonify(result), 200
    except Exception as e:
        logger.exception("Failed to compute metrics")
        return jsonify({"error": "Internal server error"}), 500
```

### Service Function
```python
def get_or_compute_metrics(
    website: str,
    db_session: Session,
    llm_clients: dict[str, LLMClient],
    logger: Logger
) -> dict:
    """Get cached metrics or compute new ones."""
    
    # Check cache
    cached = cache_service.get_cached_metrics(website, db_session)
    if cached and not is_stale(cached.updated_at, hours=24):
        logger.info("Returning cached metrics", extra={"brand_id": cached.brand_id})
        return format_response(cached, cached=True)
    
    # Run pipeline
    logger.info("Computing new metrics", extra={"website": website})
    return compute_metrics_pipeline(website, db_session, llm_clients, logger)
```

---

## API Documentation (CRITICAL)

**ALWAYS update Postman collection when you modify API endpoints!**

### When to Update Postman Collection
Any time you change API routes in `api/routes.py` or add new routes:
- Add new endpoints
- Modify existing endpoint paths or methods
- Change request parameters (query, body, headers)
- Modify response structure
- Add/remove status codes
- Update error responses

### Postman Collection Location
File: `brank-backend.postman_collection.json`

### What to Update
1. **New Endpoint**: Add new item to `item[]` array with:
   - Request details (method, URL, headers, body)
   - Description with parameters and response structure
   - Example responses for success and error cases

2. **Modified Endpoint**: Update existing item:
   - URL parameters
   - Request/response examples
   - Description

3. **Response Changes**: Update example responses in `response[]` array

4. **Variables**: Update collection variables if new parameters added

### Update Checklist
- [ ] Added/updated request in Postman collection
- [ ] Added description with parameter details
- [ ] Added example success responses (200, 201, etc.)
- [ ] Added example error responses (400, 404, 500, etc.)
- [ ] Updated collection variables if needed
- [ ] Tested import in Postman to verify JSON is valid

**⚠️ NEVER modify API endpoints without updating the Postman collection!**

**Note:** A GitHub workflow (`.github/workflows/check-postman.yml`) will automatically validate that the Postman collection is updated when API files change.

---

## Database Migrations (CRITICAL)

**ALWAYS create a migration when you modify database schema!**

### When to Create Migrations
Any time you change `db/models.py`:
- Add/remove/modify columns
- Add/remove tables
- Change column types, constraints, or indexes
- Modify relationships

### Migration Workflow (4 Steps)

1. **Edit the models** in `db/models.py`
2. **Generate migration**:
   ```bash
   uv run alembic revision --autogenerate -m "descriptive message"
   ```
3. **Review the generated file** in `alembic/versions/` - ensure it's correct
4. **Apply the migration**:
   ```bash
   uv run alembic upgrade head
   ```

### Migration Best Practices
- **Descriptive messages**: Use clear descriptions like "add user_email column to brands" not "update schema"
- **Review before applying**: Always check the auto-generated migration file
- **One logical change per migration**: Don't bundle unrelated schema changes
- **Test rollback**: Verify `downgrade()` works if you need to undo changes

### Example Migration Message
```bash
# Good
uv run alembic revision --autogenerate -m "add status column to responses table"

# Bad
uv run alembic revision --autogenerate -m "update db"
```

**⚠️ NEVER modify the database schema without creating a migration!**

**Note:** A GitHub workflow (`.github/workflows/check-migrations.yml`) will automatically validate that migrations are included when `db/models.py` changes.

---

## Testing & Quality Checks (CRITICAL)

**GitHub workflows automatically run on every PR:**

1. **`.github/workflows/run-tests.yml`** - Runs all tests
   - Sets up PostgreSQL test database
   - Runs pytest with coverage
   - Runs linting (Black, isort)
   - Runs type checking (mypy)
   - **PR will be blocked if tests fail**

2. **`.github/workflows/check-migrations.yml`** - Validates migrations
   - Checks if migration files are included with schema changes

3. **`.github/workflows/check-postman.yml`** - Validates API docs
   - Checks if Postman collection is updated with API changes

### Before Creating PR
```bash
# Run tests locally
uv run pytest -v

# Format code
uv run black .
uv run isort .

# Type check
uv run mypy .

# Generate migration if needed
uv run alembic revision --autogenerate -m "description"
uv run alembic upgrade head
```

---

## Don'ts (Never Do This)

❌ Put business logic in Flask routes
❌ Use global variables for DB connections or HTTP clients
❌ Hardcode API keys, URLs, or configuration
❌ Swallow exceptions without logging
❌ Make network calls without timeouts
❌ Commit without type hints or tests
❌ Use `import *`
❌ Create new patterns if existing ones work
❌ Modify db/models.py without creating a migration
❌ Modify API endpoints without updating Postman collection

---

## Summary Checklist

Before writing code, verify:
- [ ] I understand which layer this belongs to
- [ ] I'm using dependency injection (no globals)
- [ ] I'm following existing patterns
- [ ] I'm handling errors explicitly
- [ ] I'm adding type hints and docstrings
- [ ] I'm writing/updating tests
- [ ] I'm considering performance (timeouts, retries, parallelism)
- [ ] I'm logging important operations
- [ ] I'm not hardcoding configuration
- [ ] If I modified db/models.py, I created and applied a migration
- [ ] If I modified API endpoints, I updated the Postman collection

**Remember**: Code quality > speed. Take time to do it right the first time.

